---
title: "Regression Assignment"
author: "Jean-Paul Courneya"
date: "10/24/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The classic question confronted. Is an Automatic or Manual transmission better for MPG?
========================================================
    
## Executive summary
    
In this report will address the question : "Is automatic or manual transmission better for mpg ?". To answer this question we used a dataset from the 1974 Motor Trend US magazine, and ran some statistical tests and a regression analysis. On one hand the statistical tests show (without controlling for other car design features) a difference in mean of about 7 miles more for the manual transmitted cars. On the other hand, the regression analysis indicate that by taking into account other variables like weight and 1/4 mile time, manual transmitted cars are only 2.9 miles better than automatic transmitted cars and also that this result is less significant than to consider weight and 1/4 mile time together.

## Cleaning data

The first step of our analysis is simply to load and take a look at the data.

```{r}
data(mtcars)
str(mtcars)
```

Now we coerce the "cyl", "vs", "gear", "carb" and "am" variables into factor variables.

```{r}
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am)
```

For a better readability, we rename the levels of the "am" variable into "Auto" and "Manual".

```{r}
levels(mtcars$am) <- c("Auto", "Manual")
```

## Graphics

We begin by plotting boxplots of the variable "mpg" when "am" is "Auto" or "Manual" (see Figure 1 in the appendix). This plot hints at an increase in mpg when gearing was manual but this data may have other variables which may play a bigger role in determination of mpg.

We then plot the relationships between all the variables of the dataset (see Figure 2 in the appendix). We may note that variables like "wt", "cyl", "disp" and "hp" seem highly correlated together.

## Inference

We may also run some tests to compare the mpg means between automatic and manual transmissions.

### T-test

We begin by using a t-test assuming that the mileage data has a normal distribution.

```{r, results = 'hide'}
t.test(mpg ~ am, data = mtcars)
```

The p-value of `r t.test(mpg ~ am, data = mtcars)$p.value` clearly shows that the manual and automatic transmissions are significatively different.

### Wilcoxon test

Next we use a nonparametric Wilcoxon test to determine if there's a difference in the population means.

```{r, results = 'hide'}
wilcox.test(mpg ~ am, data = mtcars)
```

Here again the p-value of `r wilcox.test(mpg ~ am, data = mtcars)$p.value` allow us to reject the null hypothesis that the mileage data of the manual and automatic transmissions are from the same population (indicating a difference).

## Regression analysis

First we need to select a model, we proceed by using the Bayesian Information Criteria (BIC) in a stepwise algorithm. This algorithm does not evaluate the BIC for all possible models but uses a search method that compares models sequentially. Thus it bears some comparison to the classical stepwise method but with the advantage that no dubious p-values are used.

```{r results = 'hide'}
model.all <- lm(mpg ~ ., data = mtcars)
n <- nrow(mtcars)
model <- step(model.all, direction = "backward", k = log(n))
```

```{r}
summary(model)
```

The BIC algorithm tells us to consider "wt" and "qsec" as confounding variables. The individual p-values allows us to reject the hypothesis that the coefficients of "wt", "qsec" and "am" are null. The adjusted r-squared is `r summary(model)$adj.r.squared`, so we may conclude that more than `r round(summary(model)$adj.r.squared * 100)`% of the variation is explained by the model.

```{r}
anova(lm(mpg ~ am, data = mtcars), lm(mpg ~ am + qsec, data = mtcars), lm(mpg ~ am + wt + qsec, data = mtcars))
```

We may notice that when we compare the model with only "am" as independant variable and our chosen model, we reject the null hypothesis that the variables "wt" and "qsec" don't contribute to the accuracy of the model.

The regression suggests that, "wt" and "qsec" variables remaining constant, manual transmitted cars can drive `r summary(model)$coef[4]` more miles per gallon on average than automatic transmitted cars, and the results are statistically significant.

```{r}
confint(model)
```

More accurately, we are 95% confident that the difference in miles per gallon between manual and automatic transmitted cars lies somewhere in the interval [`r as.numeric(confint(model)[4, ][1])`, `r as.numeric(confint(model)[4, ][2])`].

## Residuals and diagnostics

### Residual analysis

We begin by studying the residual plots (see Figure 3 in the appendix). These plots allow us to verify some assumptions made before. We have to point that due to the small sample size 

1. The Residuals vs Fitted plot seem to verify the independance assumption as the points are randomly scattered on the plot (a Durbin-Watson test further confirms this assumption at the 0.05 level).
2. The Normal Q-Q plot seem to indicate that the residuals are normally distributed as the points hug the line closely (a Shapiro-Wilk test further confirms this assumption at the 0.05 level).
3. The Scale-Location plot seem to verify the constant variance assumption as the points fall in a constant band (a Breusch-Pagan test further confirms this assumption at the 0.05 level).

### Leverages

We begin by computing the leverages for the "mtcars" dataset.

```{r}
leverage <- hatvalues(model)
```

Are any of the observations in the dataset outliers ? We find the outliers by selecting the observations with a hatvalue > 0.5.

```{r}
leverage[which(leverage > 0.5)]
```

### Dfbetas

Next we look at the Dfbetas of the observations.

```{r}
influential <- dfbetas(model)
```

Are any of the observations in the dataset influential ? We find the influential observations by selecting the ones with a dfbeta > 1 in magnitude.

```{r}
influential[which(abs(influential) > 1)]
```

This influential observation corresponds to the Chrysler Imperial.

## Appendix

### Figure 1 : Boxplots of "mpg" vs. "am"

```{r fig.width = 10, fig.height = 10}
plot(mpg ~ am, data = mtcars, main = "Mpg by transmission type", xlab = "Transmission type", ylab = "Miles per gallon")
```

### Figure 2 : Pairs graph

```{r fig.width = 8.5, fig.height = 8.5}
panel.cor <- function(x, y, digits=2, cex.cor)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits=digits)[1]
  test <- cor.test(x,y)
  Signif <- ifelse(round(test$p.value,3)<0.001,"p<0.001",paste("p=",round(test$p.value,3)))  
  text(0.5, 0.25, paste("r=",txt))
  text(.5, .75, Signif)
}
pairs(mtcars, lower.panel = panel.smooth, upper.panel = panel.cor, main = "Pairs graph for MTCars")
```

### Figure 3 : Residual plots

```{r fig.width = 10, fig.height = 10}
par(mfrow = c(2, 2))
plot(fitted(model), residuals(model), xlab = "Fitted values", ylab = "Residuals", main = "Residuals vs Fitted")
abline(h = 0, col = "red")
qqnorm(residuals(model))
qqline(residuals(model), col = "red")
plot(fitted(model), sqrt(abs(rstandard(model))), xlab = "Fitted values", ylab = "Square Root of Standardized Residuals", main = "Scale-Location")
```

```{r}
library(lmtest)
bptest(model)
```

We do not reject the null hypothesis that the variance is the same for all observations at the 0.05 level. There is relatively weak evidence against the assumption of constant variance.

```{r}
dwtest(model, alternative = "two.sided")
```

In this case we do not reject the null hypothesis at the 0.05 level; there is very little evidence of nonzero autocorrelation in the residuals.

```{r}
shapiro.test(residuals(model))
```

We do not reject the null hypothesis that the residuals are normal at the 0.05 level.
